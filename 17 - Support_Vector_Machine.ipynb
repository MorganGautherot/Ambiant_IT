{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QVRfyCS23vOZ"},"source":["# Support Vector Machine"]},{"cell_type":"markdown","source":["# Importation des packages"],"metadata":{"id":"wCCgV3bPiNN6"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","\n","from sklearn.svm import SVR\n","from sklearn.svm import SVC\n","from sklearn.datasets import make_moons"],"metadata":{"id":"B2QMTcp3iR9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_meshgrid(x, y, h=.02):\n","    \"\"\"Create a mesh of points to plot in\n","\n","    Parameters\n","    ----------\n","    x: data to base x-axis meshgrid on\n","    y: data to base y-axis meshgrid on\n","    h: stepsize for meshgrid, optional\n","\n","    Returns\n","    -------\n","    xx, yy : ndarray\n","    \"\"\"\n","    x_min, x_max = x.min() - 1, x.max() + 1\n","    y_min, y_max = y.min() - 1, y.max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                         np.arange(y_min, y_max, h))\n","    return xx, yy\n","\n","def plot_contours(ax, clf, xx, yy, **params):\n","    \"\"\"Plot the decision boundaries for a classifier.\n","\n","    Parameters\n","    ----------\n","    ax: matplotlib axes object\n","    clf: a classifier\n","    xx: meshgrid ndarray\n","    yy: meshgrid ndarray\n","    params: dictionary of params to pass to contourf, optional\n","    \"\"\"\n","    xy = std.transform(np.c_[xx.ravel(), yy.ravel()])\n","    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    out = ax.contourf(xx, yy, Z, **params)\n","    return out"],"metadata":{"id":"CcVJWfc5lEKR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Lier votre sessions Google Colab avec Google Drive"],"metadata":{"id":"DKadkrjXleez"}},{"cell_type":"markdown","source":["Ajoutez un raccourci de ce dossier à votre google drive :\n","\n","https://drive.google.com/drive/folders/1ghsB3WdBlyLRzvfede0KWDqf9P16KyHl?usp=sharing"],"metadata":{"id":"7rnZ7LAcl3bo"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"4zPFRtoJlhtK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Génération des données des régressions"],"metadata":{"id":"ou3egTH3iT9G"}},{"cell_type":"code","source":["m = 100\n","X = 6 * np.random.rand(m, 1) - 3\n","y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n","\n","plt.scatter(X,y)\n","plt.show()"],"metadata":{"id":"_5MI68fwiXTU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Préparation des données"],"metadata":{"id":"YLetaStniae6"}},{"cell_type":"markdown","source":["Pour l'utilisation d'un modèle linéaire, il est indispensable de passer par une étape de normalisation des données.\n","\n","Cette étape permet de rendre le modèle interprétable mais aussi de faciliter la convergence du modèle.\n","\n","N'hésitez pas à utiliser le [doc](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."],"metadata":{"id":"QDPxad__idlZ"}},{"cell_type":"code","source":["std = None\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = None"],"metadata":{"id":"YQwh8LvIik1-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hUll1EwwpE5F"},"source":["## 1 Régression linéaire VS SVR"]},{"cell_type":"markdown","source":["Vous allez voir la différence entre la régresssion linéaire et le SVM."],"metadata":{"id":"_jDxGf-wjEku"}},{"cell_type":"markdown","source":["initialisation du modèle"],"metadata":{"id":"aZxZjMdbjJNq"}},{"cell_type":"markdown","source":["Dans le cas de la régression, il n'y a pas de choix d'hyperparamètre.\n","\n","Il suffit donc d'initialiser la fonction.\n","\n","N'hésitez pas à utiliser le [doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."],"metadata":{"id":"V6Xn-vDoiu4h"}},{"cell_type":"code","metadata":{"id":"g5E6s0DDpVXC"},"source":["lin_reg = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Entraînement du modèle avec la méthode `fit`."],"metadata":{"id":"JSUBWrfQixJy"}},{"cell_type":"code","source":["None"],"metadata":{"id":"h2mOedUUivdz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualisation de la performance de la régression linéaire sur les données"],"metadata":{"id":"9cz_n1d4i3Vc"}},{"cell_type":"code","source":["x_lr = np.array(np.arange(-3, 3, 0.1)).reshape(60, 1)\n","y_lr = lin_reg.predict(x_lr)\n","plt.plot(x_lr, y_lr, color='red')\n","\n","plt.scatter(X,y)\n","plt.show()"],"metadata":{"id":"ecxuthBvi7OS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zcTY-8NNpKS_"},"source":["You will see the difference between linear regression and SVM regressor"]},{"cell_type":"markdown","metadata":{"id":"OoBodOk1rsKi"},"source":["Intialisation du SVM avec la fonction `SVR` de sklearn.\n","\n","N'hésitez pas à vous aidez de la [doc](https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVR.html)"]},{"cell_type":"code","metadata":{"id":"p0y-NeMLrqMQ"},"source":["svm = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Entraîner le modèle de SVM sur les données normalisées."],"metadata":{"id":"5g16YxFtjdNL"}},{"cell_type":"code","source":["None"],"metadata":{"id":"Afpcqm1wjdDp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Regardez les performances du modèle sur les données."],"metadata":{"id":"ozokGQJPjhbL"}},{"cell_type":"code","metadata":{"id":"57zXZMSWsAj6"},"source":["x_lr = np.array(np.arange(-3, 3, 0.1)).reshape(60, 1)\n","x_lr_std = std.transform(x_lr)\n","y_lr = svm.predict(x_lr_std)\n","plt.plot(x_lr, y_lr, color='red')\n","\n","plt.scatter(X,y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ey9YLPdmtQH2"},"source":["Comme vous pouvez le constater, le SVM peut mieux s'adapter aux données que la régression linéaire."]},{"cell_type":"markdown","metadata":{"id":"5MWHs_dussi-"},"source":["## 2 Régression logistique VS SVC"]},{"cell_type":"markdown","metadata":{"id":"cYfWK1yDs8CP"},"source":["Vous verrez la différence entre un classificateur SVM et un classifieur SVM."]},{"cell_type":"markdown","source":["### Génération des données de classification"],"metadata":{"id":"5J-NdWmOj0or"}},{"cell_type":"markdown","source":["Génération du jeu de données"],"metadata":{"id":"WzPJJCLdj6oD"}},{"cell_type":"code","source":["X, y = make_moons(n_samples=100, noise=0.1)"],"metadata":{"id":"PaWXOYIvj8lL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualisation du jeu de données."],"metadata":{"id":"i9k5AZSIj5nY"}},{"cell_type":"code","metadata":{"id":"IW6LdDBAs2G9"},"source":["plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue')\n","plt.scatter(X[y==1][:,0], X[y==1][:,1], color='green')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prépration des données"],"metadata":{"id":"yYXvon02kBGw"}},{"cell_type":"markdown","source":["Standardisation des données."],"metadata":{"id":"ZK1KesDlkOPY"}},{"cell_type":"code","source":["std = None\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = None"],"metadata":{"id":"2L9tKAnUkNUa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Entraînement des modèles"],"metadata":{"id":"Wt9-JB54kJ5d"}},{"cell_type":"markdown","metadata":{"id":"aH7NnjAotCml"},"source":["Utilisez 'lbfgs' comme solver.\n","\n","N'hésitez pas à utiliser le [doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n"]},{"cell_type":"code","metadata":{"id":"6g5PlpOUs3uT"},"source":["log_reg = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Entraîner le modèle de régression logistique avec la méthode `fit`."],"metadata":{"id":"yRkhpjuhkYAZ"}},{"cell_type":"code","source":["None"],"metadata":{"id":"pnXkVnT-kbP8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualisez la performance du modèle sur les données."],"metadata":{"id":"svKP0u5ukbvT"}},{"cell_type":"code","metadata":{"id":"Ex4RZHabtOgq"},"source":["W0 = log_reg.intercept_\n","W = log_reg.coef_\n","\n","rl_x = np.array(range(-1, 3))\n","rl_y = (-1/W[0, 1]) * (rl_x * W[0, 0] + W0[0])\n","\n","plt.plot(rl_x, rl_y, c='red')\n","\n","plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue')\n","plt.scatter(X[y==1][:,0], X[y==1][:,1], color='green')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIZH0tQnu_kg"},"source":["Initialisez le modèle de SVM pour la classification grâce à la fonction `SVC` de sklearn.\n","\n","Utilisez les paramètres kernel='rbf' et gamma='scale'.\n","\n","N'hésitez pas à vous aider de la [documentation](https://scikit-learn.org/dev/modules/generated/sklearn.svm.SVC.html)."]},{"cell_type":"code","metadata":{"id":"hlwkkrvRvJaO"},"source":["svm = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Entraîner le modèle sur les données standardisées."],"metadata":{"id":"c5mpPREpk9Sc"}},{"cell_type":"code","source":["None"],"metadata":{"id":"L8jChaw7k9G5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualisation des performances du SVM."],"metadata":{"id":"QNO6a7qalIqi"}},{"cell_type":"code","metadata":{"id":"oOynNkHuxHv1"},"source":["X0, X1 = X_std[:, 0], X_std[:, 1]\n","xx, yy = make_meshgrid(X0, X1)\n","plot_contours(plt, svm, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n","plt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.xlabel('x2')\n","plt.ylabel('x1')\n","plt.xticks(())\n","plt.yticks(())\n","plt.title('SVM decision boundary')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g2WbRedE4EgS"},"source":["## 3 L'hyperparamètre C dans la classificaition avec le SVM"]},{"cell_type":"markdown","metadata":{"id":"7DXib7s54ydw"},"source":["Chargez le jeu de données"]},{"cell_type":"code","metadata":{"id":"36ik9dmw8cCV"},"source":["svm_data_1 = np.genfromtxt('/content/drive/MyDrive/Turnover_prediction/svm_data_1.txt', delimiter=',')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Préparez les données."],"metadata":{"id":"9jij1sGTmA3N"}},{"cell_type":"code","metadata":{"id":"L7C4XAyS8jUt"},"source":["# We construct the X dataset\n","X = svm_data_1[:, :2]\n","\n","# We construct the Y dataset\n","y = svm_data_1[:, 2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ihjFeA01fio8"},"source":["Standardisation des données."]},{"cell_type":"code","metadata":{"id":"8ibXhRPjfsGN"},"source":["std = None\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Wsd2bbw_lcl"},"source":["Nous commencerons par un ensemble de données d'exemples en 2D qui peuvent être séparés par une frontière linéaire. Dans cet ensemble de données, les positions des exemples positifs (indiqués par un triangle en vert) et des exemples négatifs (indiqués par un cercle en bleu) suggèrent une séparation naturelle indiquée par l'écart. Notez toutefois qu'il existe un exemple positif aberrant (un triangle en vert) à l'extrême gauche, à environ (0,1, 4,1). Dans le cadre de cet exercice, vous verrez également comment cette valeur aberrante affecte la frontière de décision du SVM."]},{"cell_type":"code","metadata":{"id":"Rriy73TS8w7h"},"source":["plt.scatter(X[y==0, 0], X[y==0, 1], label='Admitted')\n","plt.scatter(X[y==1, 0], X[y==1, 1], label='Not admitted', marker='v')\n","plt.title('Example Dataset 1', size='xx-large')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N931RqeBAfd0"},"source":["Dans cette partie de l'exercice, vous allez essayer d'utiliser différentes valeurs du paramètre C avec les SVM. De manière informelle, le paramètre C est une valeur positive qui contrôle la pénalité pour les exemples d'apprentissage mal classés. Un paramètre C élevé indique au SVM qu'il doit essayer de classer correctement tous les exemples. C joue un rôle similaire à $\\frac{1}{\\lambda}$ , où $\\lambda$ est le paramètre de régularisation que nous avons utilisé précédemment pour la régression logistique.\n","\n","\n","Nouvelle fonction de coût :\n","\n","$$J(W)=C[\\sum^{m}_{i}y^{(i)}cost_1(W^Tx{(i)})+(1-y^{(i)})cost_0(W^Tx^{(i)})]+\\frac{1}{2}\\sum^m_{i=1}w^2_j$$\n","\n","\n","$$C=\\frac{1}{\\lambda}$$"]},{"cell_type":"markdown","metadata":{"id":"9jL3_AmgkLjs"},"source":["Intialisez la fonction `SVC` et essayez différentes valeur pour le paramètre *C*."]},{"cell_type":"code","metadata":{"id":"NU80KKaB9nA7"},"source":["### Your code start here ###\n","svc = None\n","### Your code end here ###\n","\n","svc.fit(X, y)\n","\n","W0 = svc.intercept_\n","W = svc.coef_\n","\n","rl_x = np.array(range(0, 5))\n","rl_y = (-1/W[0, 1]) * (rl_x * W[0, 0] + W0[0])\n","\n","plt.plot(rl_x, rl_y, c='red')\n","\n","plt.scatter(X[y==0, 0], X[y==0, 1], label='Admitted')\n","plt.scatter(X[y==1, 0], X[y==1, 1], label='Not admitted', marker='v')\n","plt.title('SVM Decision Boundary in function of C', size='xx-large')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2jEjwfP9kUUw"},"source":["Que pensez-vous de l'impact de C dans cet exemple ?"]},{"cell_type":"markdown","metadata":{"id":"v2bemr1q0QDh"},"source":["**Autres exemples**"]},{"cell_type":"code","metadata":{"id":"tUnowXRileF9"},"source":["X, y = make_moons(n_samples=100, noise=0.1)\n","\n","plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue')\n","plt.scatter(X[y==1][:,0], X[y==1][:,1], color='green')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AN9ZqFkz1Ofw"},"source":["Jouez avec la valeur de C et les différents kernels proposés par SKlearn."]},{"cell_type":"code","metadata":{"id":"SGm-EYUilIAc"},"source":["std = StandardScaler()\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = std.fit_transform(X)\n","\n","svm = SVC(C=100, kernel='rbf', gamma='scale')\n","\n","svm.fit(X_std, y)\n","\n","\n","X0, X1 = X_std[:, 0], X_std[:, 1]\n","xx, yy = make_meshgrid(X0, X1)\n","\n","plot_contours(plt, svm, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n","plt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.xlabel('x2')\n","plt.ylabel('x1')\n","plt.xticks(())\n","plt.yticks(())\n","plt.title('SVM decision boundary')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rC7UrghjeRHy"},"source":[],"execution_count":null,"outputs":[]}]}